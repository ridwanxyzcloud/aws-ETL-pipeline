{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO # python3: python2: BytesIO\n",
    "import configparser \n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('https://github.com/ridwanxyzcloud/aws-ETL-pipeline/blob/main/config/config.ini')\n",
    "\n",
    "# Extract configuration details\n",
    "aws_access_key = config['aws']['AWS_ACCESS_KEY']\n",
    "aws_access_secret = config['aws']['AWS_ACCESS_SECRET']\n",
    "aws_region = config['aws']['AWS_REGION']\n",
    "schema_name = config['aws']['SCHEMA_NAME']\n",
    "s3_staging_dir = config['aws']['S3_STAGING_DIR']\n",
    "s3_bucket_name = config['aws']['S3_BUCKET_NAME']\n",
    "s3_output_directory = config['aws']['S3_OUTPUT_DIRECTORY']\n",
    "password = config['aws']['PASSWORD']\n",
    "host = config['aws']['HOST']\n",
    "port = config['aws']['PORT']\n",
    "dwh_iam_role_arn = config['aws']['DWH_IAM_ROLE_ARN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Athena client\n",
    "athena_client = boto3.client(\n",
    "    'athena',\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_access_secret,\n",
    "    region_name=aws_region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Athena Query function\n",
    "\n",
    "def execute_athena_query(query, database, output_location):\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': output_location\n",
    "        }\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Query Result\n",
    "\n",
    "def get_query_results(query_execution_id):\n",
    "    import time\n",
    "    \n",
    "    while True:\n",
    "        response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "        if state == 'SUCCEEDED':\n",
    "            break\n",
    "        elif state == 'FAILED':\n",
    "            raise Exception(\"Query failed\")\n",
    "        elif state == 'CANCELLED':\n",
    "            raise Exception(\"Query was cancelled\")\n",
    "        time.sleep(2)\n",
    "    result_response = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "    return result_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### it is important to give all required permissions\n",
    "- access to athena \n",
    "- access to Amazon Glue\n",
    "- access to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code executes a query in AWS Athena, waits for it to finish, downloads the results from S3, and returns them as a pandas DataFrame. The response variable holds information about the query execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict = {}\n",
    "\n",
    "# This function is defined to download the query results from Athena\n",
    "# It takes two parameters 'client: boto3.client; and response\n",
    "def download_and_query_results(client: boto3.client, query_response: Dict) -> pd.DataFrame:\n",
    "    while True:\n",
    "        try:\n",
    "            # This function only loads the first 1000 rows\n",
    "            client.get_query_results(\n",
    "                QueryExecutionId=query_response[\"QueryExecutionId\"]\n",
    "            )\n",
    "            break\n",
    "        except Exception as err:\n",
    "            if \"not yet finished\" in str(err):\n",
    "                time.sleep(0.001)\n",
    "            else:\n",
    "                raise err\n",
    "    \n",
    "    temp_file_location: str = \"athena_query_results.csv\"\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_access_secret,\n",
    "        region_name=aws_region,\n",
    "    )\n",
    "    s3_client.download_file(\n",
    "        s3_bucket_name,\n",
    "        f\"{s3_output_directory}/{query_response['QueryExecutionId']}.csv\",\n",
    "        temp_file_location,\n",
    "    )\n",
    "    return pd.read_csv(temp_file_location) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QueryExecutionId': '620b6fe4-67d5-4fe5-bab9-97b47fffe853',\n",
       " 'ResponseMetadata': {'RequestId': 'ac20ec1f-36a1-4207-9f04-69e77cf4882d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Mon, 27 May 2024 15:45:59 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '59',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'ac20ec1f-36a1-4207-9f04-69e77cf4882d'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM csv\", \n",
    "    QueryExecutionContext={\"Database\": schema_name},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": s3_staging_dir, \n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}, \n",
    "    },\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>admin2</th>\n",
       "      <th>province_state</th>\n",
       "      <th>country_region</th>\n",
       "      <th>last_update</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>confirmed</th>\n",
       "      <th>deaths</th>\n",
       "      <th>recovered</th>\n",
       "      <th>active</th>\n",
       "      <th>combined_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anhui</td>\n",
       "      <td>China</td>\n",
       "      <td>2020-01-22T17:00:00</td>\n",
       "      <td>31.826</td>\n",
       "      <td>117.226</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Anhui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "      <td>2020-01-22T17:00:00</td>\n",
       "      <td>40.182</td>\n",
       "      <td>116.414</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Beijing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chongqing</td>\n",
       "      <td>China</td>\n",
       "      <td>2020-01-22T17:00:00</td>\n",
       "      <td>30.057</td>\n",
       "      <td>107.874</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Chongqing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fujian</td>\n",
       "      <td>China</td>\n",
       "      <td>2020-01-22T17:00:00</td>\n",
       "      <td>26.079</td>\n",
       "      <td>117.987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Fujian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gansu</td>\n",
       "      <td>China</td>\n",
       "      <td>2020-01-22T17:00:00</td>\n",
       "      <td>36.061</td>\n",
       "      <td>103.834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Gansu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fips admin2 province_state country_region          last_update  latitude  \\\n",
       "0   NaN    NaN          Anhui          China  2020-01-22T17:00:00    31.826   \n",
       "1   NaN    NaN        Beijing          China  2020-01-22T17:00:00    40.182   \n",
       "2   NaN    NaN      Chongqing          China  2020-01-22T17:00:00    30.057   \n",
       "3   NaN    NaN         Fujian          China  2020-01-22T17:00:00    26.079   \n",
       "4   NaN    NaN          Gansu          China  2020-01-22T17:00:00    36.061   \n",
       "\n",
       "   longitude  confirmed  deaths  recovered  active combined_key  \n",
       "0    117.226        1.0     NaN        NaN     NaN       \"Anhui  \n",
       "1    116.414       14.0     NaN        NaN     NaN     \"Beijing  \n",
       "2    107.874        6.0     NaN        NaN     NaN   \"Chongqing  \n",
       "3    117.987        1.0     NaN        NaN     NaN      \"Fujian  \n",
       "4    103.834        NaN     NaN        NaN     NaN       \"Gansu  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "enigma_jhu = download_and_query_results(athena_client, response)\n",
    "enigma_jhu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222804, 12)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm imported data\n",
    "enigma_jhu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the same for every other tables too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129747, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM us_county\", \n",
    "    QueryExecutionContext={\"Database\": schema_name},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": s3_staging_dir, \n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}, \n",
    "    },\n",
    ")\n",
    "\n",
    "us_county = download_and_query_results(athena_client, response)\n",
    "us_county.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3754, 5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM us_states\", \n",
    "    QueryExecutionContext={\"Database\": schema_name},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": s3_staging_dir, \n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}, \n",
    "    },\n",
    ")\n",
    "\n",
    "us_states = download_and_query_results(athena_client, response)\n",
    "us_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM state_abv\", \n",
    "    QueryExecutionContext={\"Database\": schema_name},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": s3_staging_dir, \n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}, \n",
    "    },\n",
    ")\n",
    "\n",
    "static_state_abv = download_and_query_results(athena_client, response)\n",
    "static_state_abv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 6)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM countrycode\", \n",
    "    QueryExecutionContext={\"Database\": schema_name},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": s3_staging_dir, \n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}, \n",
    "    },\n",
    ")\n",
    "\n",
    "static_countrycode = download_and_query_results(athena_client, response)\n",
    "static_countrycode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3220, 5)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM countypopulation\", \n",
    "    QueryExecutionContext={\"Database\": schema_name},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": s3_staging_dir, \n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}, \n",
    "    },\n",
    ")\n",
    "\n",
    "static_countypopulation = download_and_query_results(athena_client, response)\n",
    "static_countypopulation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 25)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM us_daily\", \n",
    "    QueryExecutionContext={\"Database\": schema_name},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": s3_staging_dir, \n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}, \n",
    "    },\n",
    ")\n",
    "\n",
    "rearc_us_daily = download_and_query_results(athena_client, response)\n",
    "rearc_us_daily.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20780, 56)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM states_daily\", \n",
    "    QueryExecutionContext={\"Database\": schema_name},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": s3_staging_dir, \n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}, \n",
    "    },\n",
    ")\n",
    "\n",
    "rearc_states_daily = download_and_query_results(athena_client, response)\n",
    "rearc_states_daily.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6637, 23)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM json\", \n",
    "    QueryExecutionContext={\"Database\": schema_name},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": s3_staging_dir, \n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}, \n",
    "    },\n",
    ")\n",
    "\n",
    "rearc_us_hospital_beds = download_and_query_results(athena_client, response)\n",
    "rearc_us_hospital_beds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 18)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM us_total_latest\", \n",
    "    QueryExecutionContext={\"Database\": schema_name},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": s3_staging_dir, \n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"}, \n",
    "    },\n",
    ")\n",
    "\n",
    "rearc_us_total_latest = download_and_query_results(athena_client, response)\n",
    "rearc_us_total_latest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data and do some cleaning or transforming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col0</th>\n",
       "      <th>col1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>State</td>\n",
       "      <td>Abbreviation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      col0          col1\n",
       "0    State  Abbreviation\n",
       "1  Alabama            AL\n",
       "2   Alaska            AK"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for table static_states_abv, the header is wrong\n",
    "static_state_abv.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['State', 'Abbreviation'], dtype='object', name=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use 'iloc' slicing to change the column header \n",
    "new_header = static_state_abv.iloc[0]\n",
    "# assign it to the dataframe columns so the change is saved\n",
    "static_state_abv.columns = new_header\n",
    "\n",
    "static_state_abv.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the 'fact_covid' from the existing tables created \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26418, 13)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factCovid_1 = enigma_jhu[['fips','province_state','country_region','confirmed','deaths','recovered','active']]\n",
    "factCovid_2 = rearc_states_daily[['fips','date','positive','negative','hospitalizedcurrently','hospitalized','hospitalizeddischarged']]\n",
    "#merge the two derivative tables\n",
    "fact_covid = pd.merge(factCovid_1, factCovid_2, on='fips', how='inner')\n",
    "\n",
    "fact_covid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract 'dim_region'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dimRegion_1 = enigma_jhu[['fips','province_state','country_region','latitude','longtitude']]\n",
    "dimRegion_2 = us_county[['fips','county','state']]\n",
    "#merge \n",
    "dim_region = pd.merge(dimRegion_1,dimRegion_2, on='fips', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45101020, 7)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dim_region.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the 'dim_hospital' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6637, 9)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dim_hospital = rearc_us_hospital_beds[['fips','state_name','latitude','longtitude','hq_address','hospital_name','hospital_type','hq_city','hq_state']]\n",
    "dim_hospital.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20780 entries, 0 to 20779\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   fips    20780 non-null  int64\n",
      " 1   date    20780 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 324.8 KB\n"
     ]
    }
   ],
   "source": [
    "dim_date = rearc_states_daily[['fips','date']]\n",
    "\n",
    "# transform 'dim_date' to extract needed data and columns\n",
    "#split \n",
    "dim_date.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>20210307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20210307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fips      date\n",
       "0     2  20210307\n",
       "1     1  20210307"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_date.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20780, 5)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the 'date' column from int64 datatype to datetime format\n",
    "dim_date['date'] = pd.to_datetime(dim_date['date'], format='%Y%m%d')\n",
    "\n",
    "\n",
    "dim_date['year'] = dim_date['date'].dt.year\n",
    "dim_date['month'] = dim_date['date'].dt.month\n",
    "dim_date['day_of_week'] = dim_date['date'].dt.dayofweek\n",
    "\n",
    "# redefine the columns and assign back to 'dim_date'\n",
    "dim_date = dim_date[['fips', 'date', 'month', 'year', 'day_of_week']]\n",
    "\n",
    "dim_date.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save all outputs and transformed data to s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving collectively\n",
    "# DataFrames to save\n",
    "data_frames = {\n",
    "    'fact_covid': fact_covid,\n",
    "    'dim_hospital': dim_hospital,\n",
    "    'dim_region': dim_region,\n",
    "    'dim_date': dim_date\n",
    "}\n",
    "# Create an S3 resource\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Upload CSV files to S3 after buffering to binary format\n",
    "for df_name, df in data_frames.items():\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3.Object(s3_bucket_name, f\"{s3_output_directory}/{df_name}.csv\").put(Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"DataFrames successfully uploaded to {s3_output_directory} on Amazon S3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR use this code \n",
    "### NOTE: The second code is faster and use less resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Create S3 resource\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Function to upload DataFrame to S3\n",
    "def save_df_to_s3(df, bucket_name, file_path):\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3.Object(bucket_name, file_path).put(Body=csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames successfully uploaded to output-data on Amazon S3.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#DataFrame to S3 \n",
    "save_df_to_s3(fact_covid, s3_bucket_name, f\"{s3_output_directory}/fact_covid.csv\")\n",
    "print(f\"DataFrames successfully uploaded to {s3_output_directory} on Amazon S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames successfully uploaded to output-data on Amazon S3.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_df_to_s3(dim_hospital, s3_bucket_name, f\"{s3_output_directory}/dim_hospital.csv\")\n",
    "print(f\"DataFrames successfully uploaded to {s3_output_directory} on Amazon S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames successfully uploaded to output-data on Amazon S3.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_df_to_s3(dim_region, s3_bucket_name, f\"{s3_output_directory}/dim_region.csv\")\n",
    "print(f\"DataFrames successfully uploaded to {s3_output_directory} on Amazon S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames successfully uploaded to output-data on Amazon S3.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_df_to_s3(dim_date, s3_bucket_name, f\"{s3_output_directory}/dim_date.csv\")\n",
    "\n",
    "print(f\"DataFrames successfully uploaded to {s3_output_directory} on Amazon S3.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step is to use our model and extracted schema to create a data warehouse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"factCovid\" (\n",
      "\"index\" INTEGER,\n",
      "  \"fips\" REAL,\n",
      "  \"province_state\" TEXT,\n",
      "  \"country_region\" TEXT,\n",
      "  \"confirmed\" REAL,\n",
      "  \"deaths\" REAL,\n",
      "  \"recovered\" REAL,\n",
      "  \"active\" REAL,\n",
      "  \"date\" INTEGER,\n",
      "  \"positive\" REAL,\n",
      "  \"negative\" REAL,\n",
      "  \"hospitalizedcurrently\" REAL,\n",
      "  \"hospitalized\" REAL,\n",
      "  \"hospitalizeddischarged\" REAL\n",
      ")\n",
      "CREATE TABLE \"dimHospital\" (\n",
      "\"index\" INTEGER,\n",
      "  \"fips\" REAL,\n",
      "  \"state_name\" TEXT,\n",
      "  \"latitude\" REAL,\n",
      "  \"longtitude\" REAL,\n",
      "  \"hq_address\" TEXT,\n",
      "  \"hospital_name\" TEXT,\n",
      "  \"hospital_type\" TEXT,\n",
      "  \"hq_city\" TEXT,\n",
      "  \"hq_state\" TEXT\n",
      ")\n",
      "CREATE TABLE \"dimRegion\" (\n",
      "\"index\" INTEGER,\n",
      "  \"fips\" REAL,\n",
      "  \"province_state\" TEXT,\n",
      "  \"country_region\" TEXT,\n",
      "  \"latitude\" REAL,\n",
      "  \"longitude\" REAL,\n",
      "  \"county\" TEXT,\n",
      "  \"state\" TEXT\n",
      ")\n",
      "CREATE TABLE \"dimDate\" (\n",
      "\"index\" INTEGER,\n",
      "  \"fips\" INTEGER,\n",
      "  \"date\" TIMESTAMP,\n",
      "  \"month\" INTEGER,\n",
      "  \"year\" INTEGER,\n",
      "  \"day_of_week\" INTEGER\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# extract DataFrame Schema\n",
    "fact_covid_sql = pd.io.sql.get_schema(fact_covid.reset_index(), 'factCovid')\n",
    "dim_hospital_sql = pd.io.sql.get_schema(dim_hospital.reset_index(), 'dimHospital')\n",
    "dim_region_sql = pd.io.sql.get_schema(dim_region.reset_index(), 'dimRegion')\n",
    "dim_date_sql = pd.io.sql.get_schema(dim_date.reset_index(), 'dimDate')\n",
    "\n",
    "print(''.join(fact_covid_sql))\n",
    "print(''.join(dim_hospital_sql))\n",
    "print(''.join(dim_region_sql))\n",
    "print(''.join(dim_date_sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Redshift and Create Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "!pip install redshift_connector\n",
    "import redshift_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to redshift using the 'redshift_connector', create a cursor and create table using the extracted schema\n",
    "\n",
    "conn = redshift_connector.connect(\n",
    "    host = host,\n",
    "    database = 'aws-etl-redshift',\n",
    "    user ='admin',\n",
    "    password=password\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<redshift_connector.cursor.Cursor at 0x1354a0fd0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('''\n",
    "  CREATE TABLE \"dimDate\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" INTEGER,\n",
    "  \"date\" TIMESTAMP,\n",
    "  \"month\" INTEGER,\n",
    "  \"year\" INTEGER,\n",
    "  \"day_of_week\" INTEGER\n",
    ");  \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<redshift_connector.cursor.Cursor at 0x1354a0fd0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('''\n",
    "CREATE TABLE \"dimRegion\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" REAL,\n",
    "  \"province_state\" TEXT,\n",
    "  \"country_region\" TEXT,\n",
    "  \"latitude\" REAL,\n",
    "  \"longitude\" REAL,\n",
    "  \"county\" TEXT,\n",
    "  \"state\" TEXT\n",
    ");  \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<redshift_connector.cursor.Cursor at 0x1354a0fd0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('''\n",
    "CREATE TABLE \"dimHospital\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" REAL,\n",
    "  \"state_name\" TEXT,\n",
    "  \"latitude\" REAL,\n",
    "  \"longtitude\" REAL,\n",
    "  \"hq_address\" TEXT,\n",
    "  \"hospital_name\" TEXT,\n",
    "  \"hospital_type\" TEXT,\n",
    "  \"hq_city\" TEXT,\n",
    "  \"hq_state\" TEXT\n",
    ");  \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<redshift_connector.cursor.Cursor at 0x1354a0fd0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('''\n",
    "CREATE TABLE \"factCovid\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" REAL,\n",
    "  \"province_state\" TEXT,\n",
    "  \"country_region\" TEXT,\n",
    "  \"confirmed\" REAL,\n",
    "  \"deaths\" REAL,\n",
    "  \"recovered\" REAL,\n",
    "  \"active\" REAL,\n",
    "  \"date\" INTEGER,\n",
    "  \"positive\" REAL,\n",
    "  \"negative\" REAL,\n",
    "  \"hospitalizedcurrently\" REAL,\n",
    "  \"hospitalized\" REAL,\n",
    "  \"hospitalizeddischarged\" REAL\n",
    ");  \n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Redshift\n",
    "- Give all the required permission\n",
    "- Load data using COPY command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwh_iam_role_arn = 'arn:aws:iam::339713018722:role/redshift-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the COPY commands for each table\n",
    "copy_factCovid = f\"\"\"\n",
    "    COPY factCovid\n",
    "    FROM 's3://covid-lake-bucket/output-data/fact_covid.csv'\n",
    "    IAM_ROLE '{dwh_iam_role_arn}'\n",
    "    CSV\n",
    "    IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "copy_dimHospital = f\"\"\"\n",
    "    COPY dimHospital\n",
    "    FROM 's3://covid-lake-bucket/output-data/dim_hospital.csv'\n",
    "    IAM_ROLE '{dwh_iam_role_arn}'\n",
    "    CSV\n",
    "    IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "copy_dimDate = f\"\"\"\n",
    "    COPY dimDate\n",
    "    FROM 's3://covid-lake-bucket/output-data/dim_date.csv'\n",
    "    IAM_ROLE '{dwh_iam_role_arn}'\n",
    "    CSV\n",
    "    IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "copy_dimRegion = f\"\"\"\n",
    "    COPY dimRegion\n",
    "    FROM 's3://covid-lake-bucket/output-data/dim_region.csv'\n",
    "    IAM_ROLE '{dwh_iam_role_arn}'\n",
    "    CSV\n",
    "    IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the COPY commands\n",
    "cur.execute(copy_factCovid)\n",
    "cur.execute(copy_dimHospital)\n",
    "cur.execute(copy_dimDate)\n",
    "cur.execute(copy_dimRegion)\n",
    "\n",
    "# Close cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "print('Data loaded into Redshift Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaded Successfully "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
