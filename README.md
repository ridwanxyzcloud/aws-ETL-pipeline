# AWS ETL Pipeline
![ETL Pipeline Architecture](https://github.com/ridwanxyzcloud/aws-ETL-pipeline/blob/26ec388fd3d66188ccc9b8d489a08ed7c230ad13/data%20model/ETL-Architecture%20.jpg)
## Overview
This repository contains the code and documentation for building an ETL (Extract, Transform, Load) pipeline on the AWS Cloud using various AWS services such as Glue, Athena, Lambda, and Redshift. The primary objective of this project is to provide a scalable and efficient solution for processing, transforming, and loading data from diverse sources into a centralized data warehouse for analysis and reporting purposes.
This repository also have a comprehensive tutorial on how to use the major tools on AWS for data enginners.

## Features
- **ETL Scripts**: Python scripts for extracting, transforming, and loading data from various sources into AWS services.
- **AWS Glue Jobs**: Configuration files and scripts for defining and executing data transformation jobs using AWS Glue.
- **SQL Queries**: SQL query files for performing data analysis and querying data stored in AWS services like Redshift and Athena.
- **Documentation**: Detailed documentation covering project overview, setup instructions, architecture diagrams, and usage guidelines.
- **Tests**: Unit tests and integration tests for ensuring the reliability and correctness of the ETL pipeline components.
- **Sample Data**: Sample datasets and files used for testing and demonstration purposes.

## Getting Started
To get started with the AWS ETL pipeline, follow these steps:

1. Clone the repository to your local environment.
2. Follow the setup instructions provided in the documentation to configure AWS services and dependencies.
3. Execute the ETL pipeline scripts and jobs as per the defined workflow.
4. Refer to the documentation for troubleshooting, best practices, and additional resources.

## Documentation
Detailed documentation for the AWS ETL pipeline project can be found in the `docs/` directory. The documentation includes:
- Project Overview
- Setup Instructions
- Architecture Diagrams
- Usage Guidelines

## Libraries
- boto3
- pandas
- io / SringIO
- configparser 
- time
- redshift-connector

## Contributing
Contributions to the AWS ETL pipeline project are welcome! To contribute, follow these steps:
1. Fork the repository.
2. Make your changes and enhancements.
3. Submit a pull request for review.
4. Ensure that your contributions adhere to the project's coding standards and guidelines.

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.


## Additional Information
For additional information or inquiries, please contact [Ridwan](ridwanclouds@outlook.com).

