{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3 is used to interact with AWS through python environment\n",
    "# pandas for data manipulations and transformation on file\n",
    "import boto3\n",
    "import pandas as pd \n",
    "import psycopg2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('config/configuration files.config'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aws-etl-redshift'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get(\"DWH\",\"DWH_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = config.get('AWS', 'KEY')\n",
    "SECRET = config.get('AWS', 'SECRET')\n",
    "\n",
    "DWH_WORKGROUP_NAME = config.get('DWH', 'DWH_WORKGROUP_NAME')\n",
    "DWH_NAMESPACE_NAME = config.get('DWH', 'DWH_NAMESPACE_NAME')\n",
    "DWH_DB = config.get('DWH', 'DWH_DB')\n",
    "DWH_DB_USER = config.get('DWH', 'DWH_DB_USER')\n",
    "DWH_DB_PASSWORD = config.get('DWH', 'DWH_DB_PASSWORD')\n",
    "DWH_PORT = config.getint('DWH', 'DWH_PORT')\n",
    "DWH_IAM_ROLE_NAME = config.get('DWH', 'DWH_IAM_ROLE_NAME')\n",
    "REGION = config.get(\"DWH\",\"REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EC2 resource\n",
    "ec2 = boto3.resource('ec2',\n",
    "                     region_name=REGION,\n",
    "                     aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET)\n",
    "\n",
    "# Create S3 resource\n",
    "s3 = boto3.resource('s3',\n",
    "                    region_name=REGION,\n",
    "                    aws_access_key_id=KEY,\n",
    "                    aws_secret_access_key=SECRET)\n",
    "\n",
    "# Create IAM client\n",
    "iam = boto3.client('iam',\n",
    "                   region_name=REGION,\n",
    "                   aws_access_key_id=KEY,\n",
    "                   aws_secret_access_key=SECRET)\n",
    "\n",
    "# Create Redshift client\n",
    "redshift = boto3.client('redshift-serverless',\n",
    "                        region_name=REGION,\n",
    "                        aws_access_key_id=KEY,\n",
    "                        aws_secret_access_key=SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['compressed_customers.csv.gz',\n",
       " 'compressed_products.csv.gz',\n",
       " 'compressed_sales.csv.gz']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing the bucket\n",
    "\n",
    "# Define the bucket name\n",
    "bucket_name = 'ridwanclouds-bucket'\n",
    "\n",
    "# Access the bucket\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "# List all items in the bucket using an empty prefix\n",
    "log_data_file = [filename.key for filename in bucket.objects.filter(Prefix='')]\n",
    "log_data_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workgroup Info:\n",
      "{'workgroup': {'baseCapacity': 16, 'configParameters': [{'parameterKey': 'auto_mv', 'parameterValue': 'true'}, {'parameterKey': 'datestyle', 'parameterValue': 'ISO, MDY'}, {'parameterKey': 'enable_case_sensitive_identifier', 'parameterValue': 'false'}, {'parameterKey': 'enable_user_activity_logging', 'parameterValue': 'true'}, {'parameterKey': 'query_group', 'parameterValue': 'default'}, {'parameterKey': 'require_ssl', 'parameterValue': 'false'}, {'parameterKey': 'search_path', 'parameterValue': '$user, public'}, {'parameterKey': 'use_fips_ssl', 'parameterValue': 'false'}, {'parameterKey': 'max_query_execution_time', 'parameterValue': '14400'}], 'creationDate': datetime.datetime(2024, 5, 22, 13, 38, 23, 869000, tzinfo=tzutc()), 'endpoint': {'address': 'default-workgroup.339713018722.us-east-1.redshift-serverless.amazonaws.com', 'port': 5439, 'vpcEndpoints': [{'networkInterfaces': [{'availabilityZone': 'us-east-1b', 'networkInterfaceId': 'eni-0756a60271051dbdb', 'privateIpAddress': '172.31.83.10', 'subnetId': 'subnet-0cebff801df3935f1'}, {'availabilityZone': 'us-east-1f', 'networkInterfaceId': 'eni-0c1709715c765337c', 'privateIpAddress': '172.31.72.33', 'subnetId': 'subnet-012b67d19aa216337'}], 'vpcEndpointId': 'vpce-0f3b16da016428d52', 'vpcId': 'vpc-0250598562a53d67c'}]}, 'enhancedVpcRouting': False, 'namespaceName': 'default-namespace', 'patchVersion': '180', 'publiclyAccessible': True, 'securityGroupIds': ['sg-0a9be482d2c0694af'], 'status': 'AVAILABLE', 'subnetIds': ['subnet-0cebff801df3935f1', 'subnet-01cbc992a0bfb5f01', 'subnet-012b67d19aa216337'], 'workgroupArn': 'arn:aws:redshift-serverless:us-east-1:339713018722:workgroup/3dd20e4c-06a5-48ac-aa14-af79502e3b0f', 'workgroupId': '3dd20e4c-06a5-48ac-aa14-af79502e3b0f', 'workgroupName': 'default-workgroup', 'workgroupVersion': '1.0.63282'}, 'ResponseMetadata': {'RequestId': '74514862-576a-4761-be48-f5a8ed0908fe', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '74514862-576a-4761-be48-f5a8ed0908fe', 'date': 'Fri, 24 May 2024 14:25:03 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1737', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "#workgroup info \n",
    "try:\n",
    "    workgroup_info = redshift.get_workgroup(workgroupName=DWH_WORKGROUP_NAME)\n",
    "    print(\"Workgroup Info:\")\n",
    "    print(workgroup_info)\n",
    "except Exception as e:\n",
    "    workgroup_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workgroup</th>\n",
       "      <th>ResponseMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseCapacity</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>configParameters</th>\n",
       "      <td>[{'parameterKey': 'auto_mv', 'parameterValue':...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          workgroup  \\\n",
       "baseCapacity                                                     16   \n",
       "configParameters  [{'parameterKey': 'auto_mv', 'parameterValue':...   \n",
       "\n",
       "                 ResponseMetadata  \n",
       "baseCapacity                  NaN  \n",
       "configParameters              NaN  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workgroup_info_df = pd.DataFrame(workgroup_info)\n",
    "workgroup_info_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace Info:\n",
      "{'namespace': {'adminUsername': 'admin', 'creationDate': datetime.datetime(2024, 5, 22, 13, 38, 23, 349000, tzinfo=tzutc()), 'dbName': 'dev', 'iamRoles': [], 'kmsKeyId': 'AWS_OWNED_KMS_KEY', 'logExports': [], 'namespaceArn': 'arn:aws:redshift-serverless:us-east-1:339713018722:namespace/02da2352-e369-4565-b22d-83b652e65ab3', 'namespaceId': '02da2352-e369-4565-b22d-83b652e65ab3', 'namespaceName': 'default-namespace', 'status': 'AVAILABLE'}, 'ResponseMetadata': {'RequestId': '03a714e1-0513-4cb0-8661-47c45e73c4fe', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '03a714e1-0513-4cb0-8661-47c45e73c4fe', 'date': 'Fri, 24 May 2024 14:25:19 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '382', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Describe the namespace\n",
    "try:\n",
    "    namespace_info = redshift.get_namespace(namespaceName=DWH_NAMESPACE_NAME)\n",
    "    print(\"Namespace Info:\")\n",
    "    print(namespace_info)\n",
    "except Exception as e:\n",
    "    print(f\"Error describing namespace: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Relation shoes does not exist in the database.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing the redshift to be sure the allocated role is active and effective \n",
    "\n",
    "workgroup_endpoint = 'default-workgroup.339713018722.us-east-1.redshift-serverless.amazonaws.com'\n",
    "\n",
    "# Connect to the Redshift Serverless endpoint to list databases\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DWH_DB,\n",
    "        user=DWH_DB_USER,\n",
    "        password=DWH_DB_PASSWORD,\n",
    "        host=workgroup_endpoint,\n",
    "        port=DWH_PORT\n",
    "    )\n",
    "    # Execute SQL query\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('SELECT * FROM \"aws-etl-redshift\".\"public\".\"shoes\"')\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    # Print results\n",
    "    print(\"Results:\")\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "\n",
    "    # Close cursor and connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully to S3 bucket: ridwanclouds-bucket\n",
      "File uploaded successfully to S3 bucket: ridwanclouds-bucket\n",
      "File uploaded successfully to S3 bucket: ridwanclouds-bucket\n"
     ]
    }
   ],
   "source": [
    "## Upload the compressed data files into s3 bucket \n",
    "\n",
    "# defining S3 bucket name and file paths\n",
    "bucket_name = 'ridwanclouds-bucket'\n",
    "compressed_customer_csv_path = \"/Users/villy/Documents/GitHub/aws-ETL-pipeline/data/compressed_customers.csv.gz\"\n",
    "compressed_product_csv_path = \"/Users/villy/Documents/GitHub/aws-ETL-pipeline/data/compressed_products.csv.gz\"\n",
    "compressed_sales_csv_path = \"/Users/villy/Documents/GitHub/aws-ETL-pipeline/data/compressed_sales.csv.gz\"\n",
    "\n",
    "\n",
    "\n",
    "# Function Uploading compressed CSV files to S3 bucket\n",
    "def upload_to_s3(file_path, bucket_name, object_name):\n",
    "    try:\n",
    "        s3.meta.client.upload_file(file_path, bucket_name, object_name)\n",
    "        print(f\"File uploaded successfully to S3 bucket: {bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3 bucket: {e}\")\n",
    "\n",
    "# Uploading compressed CSV files\n",
    "upload_to_s3(compressed_customer_csv_path, bucket_name, 'compressed_customers.csv.gz')\n",
    "upload_to_s3(compressed_product_csv_path, bucket_name, 'compressed_products.csv.gz')\n",
    "upload_to_s3(compressed_sales_csv_path, bucket_name, 'compressed_sales.csv.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using COPY command for ETL process \n",
    "\n",
    "- The COPY command is used to load data into DW on Redshift. It is seemlesly easy and faster as the compressed data is already \n",
    "located in an S3 bucket as an object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(\n",
    "        dbname=DWH_DB,\n",
    "        user=DWH_DB_USER,\n",
    "        password=DWH_DB_PASSWORD,\n",
    "        host=workgroup_endpoint,\n",
    "        port=DWH_PORT\n",
    "    )\n",
    "cur = conn.cursor()\n",
    "cur.execute('''SELECT * FROM \"aws-etl-redshift\".\"public\".\"customers\";''')\n",
    "result = cur.fetchall()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: cross-database reference to database \"aws-etl-redshift\" is not supported\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connect to the Redshift Serverless endpoint\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DWH_DB,\n",
    "        user=DWH_DB_USER,\n",
    "        password=DWH_DB_PASSWORD,\n",
    "        host=workgroup_endpoint,\n",
    "        port=DWH_PORT\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Copy data into customers table\n",
    "    copy_customers = f\"\"\"\n",
    "    COPY customers\n",
    "    FROM 's3://s3://ridwanclouds-bucket/compressed_customers.csv.gz'\n",
    "    IAM_ROLE '{DWH_IAM_ROLE_NAME}'\n",
    "    CSV\n",
    "    GZIP\n",
    "    IGNOREHEADER 1;\n",
    "    \"\"\"\n",
    "    cur.execute(copy_customers)\n",
    "\n",
    "    # Copy data into products table\n",
    "    copy_products = f\"\"\"\n",
    "    COPY products\n",
    "    FROM 's3://s3://ridwanclouds-bucket/compressed_products.csv.gz'\n",
    "    IAM_ROLE '{DWH_IAM_ROLE_NAME}'\n",
    "    CSV\n",
    "    GZIP\n",
    "    IGNOREHEADER 1;\n",
    "    \"\"\"\n",
    "    cur.execute(copy_products)\n",
    "\n",
    "    # Copy data into sales table\n",
    "    copy_sales = f\"\"\"\n",
    "    COPY sales\n",
    "    FROM 's3://ridwanclouds-bucket/compressed_sales.csv.gz'\n",
    "    IAM_ROLE '{DWH_IAM_ROLE_NAME}'\n",
    "    CSV\n",
    "    GZIP\n",
    "    IGNOREHEADER 1;\n",
    "    \"\"\"\n",
    "    cur.execute(copy_sales)\n",
    "\n",
    "    # Commit the transactions\n",
    "    conn.commit()\n",
    "\n",
    "    # Close cursor and connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
